# -*- coding: utf-8 -*-
"""Credit Card Default Prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ptMlUfY7jTlmXqKDkL3ebpy3wPx3t827

# **Problem Description**

This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card payments.

# **Data Description**

This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following variables as explanatory variables:

**ID:** Unique ID of each client

**LIMIT_BAL:** Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.

**Gender:** 1 = male; 2 = female

**Education:** 1 = graduate school; 2 = university; 3 = high school; 4 = others

**Marital status:** 1 = married; 2 = single; 3 = others).

**Age:** Age in years
# History of past payment.
We tracked the past monthly payment records from April to September, 2005.The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.

**PAY_0:** Repayment status in September, 2005

**PAY_2:** Repayment status in August, 2005

**PAY_3:** Repayment status in July, 2005

**PAY_4:** Repayment status in June, 2005

**PAY_5:** Repayment status in May, 2005

**PAY_6:** Repayment status in April, 2005

# Amount of bill statement (NT dollar).

**BILL_AMT1:** Amount of bill statement in September, 2005

**BILL_AMT2:** Amount of bill statement in August, 2005

**BILL_AMT3:** Amount of bill statement in July, 2005

**BILL_AMT4:** Amount of bill statement in June, 2005

**BILL_AMT5:** Amount of bill statement in May, 2005

**BILL_AMT6:** Amount of bill statement in April, 2005

# Amount of previous payment (NT dollar).

**PAY_AMT1:** Amount of previous payment in September, 2005

**PAY_AMT2:** Amount of previous payment in August, 2005

**PAY_AMT3:** Amount of previous payment in July, 2005

**PAY_AMT4**: Amount of previous payment in June, 2005

**PAY_AMT5**: Amount of previous payment in May, 2005

**PAY_AMT6**: Amount of previous payment in April, 2005

**default.payment.next.month:** Default payment (1=yes, 0=no)

# **LET'S BEGIN!!!**

**Importing Libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
#importing required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

"""**Loading the Dataset**"""

from google.colab import drive
drive.mount('/content/drive')
# Importing the dataset
filepath= "/content/drive/MyDrive/Copy of default of credit card clients.xls"
data = pd.read_excel(filepath )
data.head(5)

#to display upto 200 columns and rows at once
pd.set_option('display.max_columns', 200)
pd.set_option('display.max_rows', 200)

data=data.rename(columns=data.iloc[0]).reset_index(drop=True) #removing the first row and setting the second row as column names
data.head(5)

"""**Understanding the Data**"""

data.shape

data.size

data.columns

data.dtypes

data.info()

data.describe()

data.isna().sum()

data.duplicated().sum()

"""There are no null values or duplicate entries, The data is already clean.

**Data Pre-processing**
"""

#renaming of columns for easier understanding and simplicity
data.rename(columns={'default payment next month' : 'IsDefaulter'}, inplace=True)
data.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)
data.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)
data.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'}, inplace= True)
data.head(5)

data.drop(0, axis=0, inplace= True)
data.head(5)

#replacing values with there respective labels
data.replace({'SEX': {1 : 'Male', 2 : 'Female'}}, inplace=True)
data.replace({'EDUCATION' : {1 : 'Graduate School', 2 : 'University', 3 : 'High School', 4 : 'Others'}}, inplace=True)
data.replace({'MARRIAGE' : {1 : 'Married', 2 : 'Single', 3 : 'Others'}}, inplace = True)
data.replace({'IsDefaulter' : {1 : 'Yes', 0 : 'No'}}, inplace = True)
#check for replaced labels
data.head(5)

"""# **Exploratory Data Analysis**

# Defaulter
"""

plt.figure(figsize=(5,4))
sns.countplot(x= 'IsDefaulter', data= data)

e=[0.1,0]
data['IsDefaulter'].value_counts().plot.pie(autopct='%1.1f%%', shadow= True, explode=e)

"""About 22% of customers are defaulters.

# Gender
"""

data['SEX'].value_counts().plot.pie(autopct='%1.1f%%', shadow= True, explode=e, )

"""There are significantly higher numbers Female customers than Male customers"""

plt.figure(figsize=(5,4))
sns.countplot(x='SEX', hue='IsDefaulter', data= data)

"""By overall numbers, Female has higher numbers of defaults but they are also higher number of female customers. """

#Default rate among Males and Females
data[data['SEX']=='Male']['IsDefaulter'].value_counts().plot.pie(autopct='%1.1f%%', shadow= True, explode=e)
plt.title('Default rate among Males')

data[data['SEX']=='Female']['IsDefaulter'].value_counts().plot.pie(autopct='%1.1f%%', shadow= True, explode=e)
plt.title('Default rate among Females')

"""Males have slightly higher rates of default than females.

# Education
"""

#Education background of customers
data['EDUCATION'].value_counts()

"""Dataset has no description of 0,5,6 only in the education column, I think it should be replaced to 'Others'."""

#replcae values with 5, 6 and 0 to Others
data['EDUCATION'] = data['EDUCATION'].replace({5: "Others", 6: "Others",0: "Others"})

#count plot for EDUCATION and with respect to IsDefaulter
fig, axes = plt.subplots(ncols=2,figsize=(15,4))
sns.countplot(x = 'EDUCATION', ax = axes[0], data = data)
sns.countplot(x = 'EDUCATION', hue = 'IsDefaulter',ax = axes[1], data = data)

#Default rate among customers of several educational backgrounds

data[data['IsDefaulter']=='Yes']['EDUCATION'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Default rate among customers of various educational backgrounds ')

#Educational backgrounds of customers
data['EDUCATION'].value_counts().plot.pie(autopct='%1.1f%%')
plt.title('Educational backgrounds of Customers')

"""People with University and High School level educational background has higher default rate than their Share of population.

# Marriage
"""

#category wise values
data['MARRIAGE'].value_counts()

"""In marriage column, 0 values are not known. Let's combine those values in others category."""

#replacing 0 with Others
data.MARRIAGE = data.MARRIAGE.replace({0: "Others"})

#count plot for MARRIAGE and with respect to IsDefaulter
fig, axes = plt.subplots(ncols=2,figsize=(8,4))
sns.countplot(x = 'MARRIAGE', ax = axes[0], data = data)
sns.countplot(x = 'MARRIAGE', hue = 'IsDefaulter',ax = axes[1], data = data)

#Defaulters among customers of various Marital Status
e=[0.1,0.1,0]
data[data['IsDefaulter']=='Yes']['MARRIAGE'].value_counts().plot.pie(autopct='%1.1f%%',shadow= True, explode=e)
plt.title('Default rate among customers of various Marital Status')

"""About 50% of defaulters are single while 48% are married."""

# Defaulte rate among customers of various marital status
e=[0.1,0.1]
data[data['MARRIAGE']=='Married']['IsDefaulter'].value_counts().plot.pie(autopct='%1.1f%%',shadow= True, explode=e)
plt.title('Default rate among married customers')

data[data['MARRIAGE']=='Single']['IsDefaulter'].value_counts().plot.pie(autopct='%1.1f%%',shadow= True, explode=e)
plt.title('Default rate among single customers')

"""Default rate is slightly higher among Married customers.

# AGE
"""

#values count for AGE 
plt.figure(figsize=(20,8))
sns.countplot(x = 'AGE', data = data)

#values count for Age with respect to IsDefaulter
plt.figure(figsize=(20,8))
sns.countplot(x = 'AGE', hue = 'IsDefaulter', data = data)

"""Most Customers are from Ages ranging from 25 to 34.

# Label Encoding
"""

l_enc = {"IsDefaulter":{"Yes":1, "No":0}}
data = data.replace(l_enc)

#check for changed labels
data.head()

"""One-Hot Encoding"""

#creating dummy variables
data = pd.get_dummies(data, columns = ['SEX', 'EDUCATION', 'MARRIAGE'])
data.head(5)

data.drop(['EDUCATION_Others', 'MARRIAGE_Others'], axis=1, inplace=True)
data.shape

"""# Handling Class Imbalence

**SMOTE-** Synthetic Minority Oversampling Technique
"""

#importing SMOTE to handle class imbalance
from imblearn.over_sampling import SMOTE

smote = SMOTE()

# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(data[(i for i in list(data.describe(include='all').columns) if i != 'IsDefaulter')], data['IsDefaulter'])

print('Original unbalanced dataset shape', len(data))
print('Resampled balanced dataset shape', len(y_smote))

#creating new dataframe from balanced dataset after SMOTE
df = pd.DataFrame(x_smote, columns=list(i for i in list(data.describe(include='all').columns) if i != 'IsDefaulter'))
#adding target variable to new created dataframe
df['IsDefaulter'] = y_smote

#check for class imbalance
plt.figure(figsize=(4,4))
df['IsDefaulter'].value_counts().plot.bar(color=['red', 'maroon'])

df.shape

#removing feature ID from dataset
df.drop('ID',axis = 1, inplace = True)

"""# Correlation between all the columns"""

df.head(5)

#checking for data types
df.dtypes

#Creating a lict of columns whose data types has to be changed
col_to_convert= ['LIMIT_BAL', 'AGE', 'PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN','PAY_MAY', 'PAY_APR', 'BILL_AMT_SEPT', 'BILL_AMT_AUG', 'BILL_AMT_JUL','BILL_AMT_JUN', 'BILL_AMT_MAY', 'BILL_AMT_APR', 'PAY_AMT_SEPT','PAY_AMT_AUG', 'PAY_AMT_JUL', 'PAY_AMT_JUN', 'PAY_AMT_MAY','PAY_AMT_APR']
df[col_to_convert] = df[col_to_convert].astype(int) #Converting the dtypes to integers

# Check the data types after conversion
print(df.dtypes)

df.corr()

#seperating dependant and independant variabales and defining X any y
X = df[(list(i for i in list(df.describe(include='all').columns) if i != 'IsDefaulter'))]
y = df['IsDefaulter']

X.shape

y.shape

"""# Data Transformation"""

#importing libraries for data transformation
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

"""# Train-Test split"""

#importing libraries for splitting data into training and testing dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42, stratify=y)

X_train.shape

X_test.shape

"""# **Machine Learning Model Implementation**

# Logistic Regression
"""

#importing logistic regression and evaluation metrics
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc

#fitting data into Logistic Regression
logi = LogisticRegression()
logi.fit(X_train,y_train)

#class prediction of y
y_pred_logi = logi.predict(X_test)
y_train_pred_logi=logi.predict(X_train)

#getting all scores for Logistic Regression
train_accuracy_logi = round(accuracy_score(y_train_pred_logi,y_train), 3)
accuracy_logi = round(accuracy_score(y_pred_logi,y_test), 3)
precision_score_logi = round(precision_score(y_pred_logi,y_test), 3)
recall_score_logi = round(recall_score(y_pred_logi,y_test), 3)
f1_score_logi = round(f1_score(y_pred_logi,y_test), 3)
roc_score_logi = round(roc_auc_score(y_pred_logi,y_test), 3)

print("The accuracy on train data is ", train_accuracy_logi)
print("The accuracy on test data is ", accuracy_logi)
print("The precision on test data is ", precision_score_logi)
print("The recall on test data is ", recall_score_logi)
print("The f1 on test data is ", f1_score_logi)
print("The roc_score on test data is ", roc_score_logi)

"""Train accuracy and Test accuracy have almost equal accuracy score which means there are no over-fitting."""

# Get the confusion matrix
labels = ['Not Defaulter', 'Defaulter']
cm_logi = confusion_matrix(y_test, y_pred_logi )
print(cm_logi)

#plot confusion matrix
ax= plt.subplot()
sns.heatmap(cm_logi, annot=True, ax = ax)

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - Logistic Regression')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""# Decision Tree Classification"""

#importing Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier

#fitting data into Decision Tree Classifier
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

#class prediction of y
y_pred_dtc = dtc.predict(X_test)
y_train_pred_dtc=dtc.predict(X_train)

#getting all scores for Decision Tree Classifier
train_accuracy_dtc = round(accuracy_score(y_train_pred_dtc,y_train), 3)
accuracy_dtc = round(accuracy_score(y_pred_dtc,y_test), 3)
precision_score_dtc = round(precision_score(y_pred_dtc,y_test), 3)
recall_score_dtc = round(recall_score(y_pred_dtc,y_test), 3)
f1_score_dtc = round(f1_score(y_pred_dtc,y_test), 3)
roc_score_dtc = round(roc_auc_score(y_pred_dtc,y_test), 3)

print("The accuracy on train data is ", train_accuracy_dtc)
print("The accuracy on test data is ", accuracy_dtc)
print("The precision on test data is ", precision_score_dtc)
print("The recall on test data is ", recall_score_dtc)
print("The f1 on test data is ", f1_score_dtc)
print("The roc_score on test data is ", roc_score_dtc)

"""The accuracy score of train and test data clearly shows overfitting, we need to do hyper-parameter tuning and cross-validation."""

# Get the confusion matrix for decision tree classifier
labels = ['Not Defaulter', 'Defaulter']
cm_dtc = confusion_matrix(y_test, y_pred_dtc )
print(cm_dtc)

#plot confusion matrix
ax= plt.subplot()
sns.heatmap(cm_dtc, annot=True, ax = ax)

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - Decision Tree')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""# Hyper-Parameter Tuning and Cross-Validation"""

# The maximum depth of the tree
depth_of_tree = [20,25,30,35]

# The minimum number of samples required to split an internal node
min_samples_split = [0.001,0.01,0.05]

# Minimum number of samples required at each leaf node
min_samples_leaf = [40,50,60]

# Hyperparameter Grid
param_dict = {'max_depth': depth_of_tree,'min_samples_split':min_samples_split,'min_samples_leaf': min_samples_leaf}

# Create an instance of the decision tree
dtc = DecisionTreeClassifier()
#Finding the best parameters
from sklearn.model_selection import GridSearchCV
# Grid search
dtc_grid = GridSearchCV(estimator=dtc, param_grid = param_dict, cv = 5, verbose=3, n_jobs = -1, scoring='roc_auc')
# fitting model
dtc_grid.fit(X_train, y_train)

dtc_grid.best_estimator_

dtc_grid.best_params_

dtc_optimal_model = dtc_grid.best_estimator_

#class prediction of y on train and test
y_pred_dtc_grid=dtc_optimal_model.predict(X_test)
y_train_pred_dtc_grid=dtc_optimal_model.predict(X_train)

#getting all scores for decision tree after CV and Hyperparameter Tunning
train_accuracy_dtc_grid = round(accuracy_score(y_train_pred_dtc_grid,y_train), 3)
accuracy_dtc_grid = round(accuracy_score(y_pred_dtc_grid,y_test), 3)
precision_score_dtc_grid = round(precision_score(y_pred_dtc_grid,y_test), 3)
recall_score_dtc_grid = round(recall_score(y_pred_dtc_grid,y_test), 3)
f1_score_dtc_grid = round(f1_score(y_pred_dtc_grid,y_test), 3)
auc_dtc_grid = round(roc_auc_score(y_pred_dtc_grid,y_test), 3)

print("The accuracy on train data is ", train_accuracy_dtc_grid)
print("The accuracy on test data is ", accuracy_dtc_grid)
print("The precision on test data is ", precision_score_dtc_grid)
print("The recall on test data is ", recall_score_dtc_grid)
print("The f1 on test data is ", f1_score_dtc_grid)
print("The auc on test data is ", auc_dtc_grid)

"""The problem of Over-fitting is solved

# Random Forest Classifier
"""

#importing Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

#fitting data into Random Forest Classifier
rfc=RandomForestClassifier(n_estimators=50)
rfc.fit(X_train, y_train)

#class prediction of y
y_pred_rfc=rfc.predict(X_test)
y_train_pred_rfc=rfc.predict(X_train)

#getting all scores for Random Forest Classifier
train_accuracy_rfc = round(accuracy_score(y_train_pred_rfc,y_train), 3)
accuracy_rfc = round(accuracy_score(y_pred_rfc,y_test), 3)
precision_score_rfc = round(precision_score(y_pred_rfc,y_test), 3)
recall_score_rfc = round(recall_score(y_pred_rfc,y_test), 3)
f1_score_rfc = round(f1_score(y_pred_rfc,y_test), 3)
roc_score_rfc = round(roc_auc_score(y_pred_rfc,y_test), 3)

print("The accuracy on train data is ", train_accuracy_rfc)
print("The accuracy on test data is ", accuracy_rfc)
print("The precision on test data is ", precision_score_rfc)
print("The recall on test data is ", recall_score_rfc)
print("The f1 on test data is ", f1_score_rfc)
print("The roc_score on test data is ", roc_score_rfc)

"""Again we are facing over-fitting issue, we will have to perform Hyper-Parameter tuning and cross-validation."""

# Get the confusion matrix for Random Forest Classifier
labels = ['Not Defaulter', 'Defaulter']
cm_rfc = confusion_matrix(y_test, y_pred_rfc )
print(cm_rfc)

#plot confusion matrix
ax= plt.subplot()
sns.heatmap(cm_rfc, annot=True, ax = ax)

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - Random Forest Classifier')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""# Hyper-Parameter Tuning and Cross-validation."""

# Number of trees
n_estimators = [100,150,200]

# Maximum depth of trees
max_depth = [10,20,30]

# Minimum number of samples required to split a node
min_samples_split = [50,100,150]

# Minimum number of samples required at each leaf node
min_samples_leaf = [40,50]

# Hyperparameter Grid
param_dict = {'n_estimators' : n_estimators,'max_depth' : max_depth,'min_samples_split' : min_samples_split,'min_samples_leaf' : min_samples_leaf}
# Create an instance of the RandomForestClassifier
rfc = RandomForestClassifier()

# Grid search
rfc_grid = GridSearchCV(estimator=rfc,param_grid = param_dict,cv = 5, verbose=2, scoring='roc_auc')
# fitting model
rfc_grid.fit(X_train,y_train)

rfc_grid.best_estimator_

rfc_grid.best_params_

rfc_optimal_model = rfc_grid.best_estimator_

#class prediction of y on train and test
y_pred_rfc_grid=rfc_optimal_model.predict(X_test)
y_train_pred_rfc_grid=rfc_optimal_model.predict(X_train)

#getting all scores for Random Forest Classifier after CV and Hyperparameter Tunning
train_accuracy_rfc_grid = round(accuracy_score(y_train_pred_rfc_grid,y_train), 3)
accuracy_rfc_grid = round(accuracy_score(y_pred_rfc_grid,y_test), 3)
precision_score_rfc_grid = round(precision_score(y_pred_rfc_grid,y_test), 3)
recall_score_rfc_grid = round(recall_score(y_pred_rfc_grid,y_test), 3)
f1_score_rfc_grid = round(f1_score(y_pred_rfc_grid,y_test), 3)
auc_rfc_grid = round(roc_auc_score(y_pred_rfc_grid,y_test), 3)

print("The accuracy on train data is ", train_accuracy_rfc_grid)
print("The accuracy on test data is ", accuracy_rfc_grid)
print("The precision on test data is ", precision_score_rfc_grid)
print("The recall on test data is ", recall_score_rfc_grid)
print("The f1 on test data is ", f1_score_rfc_grid)
print("The auc on test data is ", auc_rfc_grid)

"""# Support Vector Machine"""

#importing Support Vector Classifier
from sklearn.svm import SVC

#fitting data into Support Vector Classifier
svm=SVC(probability=True)
svm.fit(X_train,y_train)

#class prediction of y
y_pred_svm=svm.predict(X_test)
y_train_pred_svm=svm.predict(X_train)

#getting all scores for Support Vector Classifier
train_accuracy_svm = round(accuracy_score(y_train_pred_svm,y_train), 3)
accuracy_svm = round(accuracy_score(y_pred_svm,y_test), 3)
precision_score_svm = round(precision_score(y_pred_svm,y_test), 3)
recall_score_svm = round(recall_score(y_pred_svm,y_test), 3)
f1_score_svm = round(f1_score(y_pred_svm,y_test), 3)
roc_score_svm = round(roc_auc_score(y_pred_svm,y_test), 3)

print("The accuracy on train data is ", train_accuracy_svm)
print("The accuracy on test data is ", accuracy_svm)
print("The precision on test data is ", precision_score_svm)
print("The recall on test data is ", recall_score_svm)
print("The f1 on test data is ", f1_score_svm)
print("The roc_score on test data is ", roc_score_svm)

# Get the confusion matrix for Support Vector Classifier
labels = ['Not Defaulter', 'Defaulter']
cm_svm = confusion_matrix(y_test, y_pred_svm )
print(cm_svm)

#plot confusion matrix
ax= plt.subplot()
sns.heatmap(cm_svm, annot=True, ax = ax)

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - SVM')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""# Hyper-Parameter Tuning and Cross-Validation"""

from sklearn.model_selection import RandomizedSearchCV

# Hyperparameter Grid
param_dict = {'C':[1, 10] ,'kernel': ['rbf']}
# Create an instance of the support vector classifier
svm=SVC(probability=True)

# Grid search
svm_grid = RandomizedSearchCV(estimator = svm, param_distributions = param_dict, cv = 2, verbose=2, n_jobs = -1, scoring= 'roc_auc')
# fitting model
svm_grid.fit(X_train, y_train)

svm_grid.best_estimator_

svm_grid.best_params_

svm_optimal_model = svm_grid.best_estimator_

#class prediction of y on train and test
y_pred_svm_grid=svm_optimal_model.predict(X_test)
y_train_pred_svm_grid=svm_optimal_model.predict(X_train)

#getting all scores for Support Vector Classifier after CV and Hyperparameter Tunning
train_accuracy_svm_grid = round(accuracy_score(y_train_pred_svm_grid,y_train), 3)
accuracy_svm_grid = round(accuracy_score(y_pred_svm_grid,y_test), 3)
precision_score_svm_grid = round(precision_score(y_pred_svm_grid,y_test), 3)
recall_score_svm_grid = round(recall_score(y_pred_svm_grid,y_test), 3)
f1_score_svm_grid = round(f1_score(y_pred_svm_grid,y_test), 3)
auc_svm_grid = round(roc_auc_score(y_pred_svm_grid,y_test), 3)

print("The accuracy on train data is ", train_accuracy_svm_grid)
print("The accuracy on test data is ", accuracy_svm_grid)
print("The precision on test data is ", precision_score_svm_grid)
print("The recall on test data is ", recall_score_svm_grid)
print("The f1 on test data is ", f1_score_svm_grid)
print("The auc on test data is ", auc_svm_grid)

"""# K-Nearest Neighbour Classifier"""

#importing KNN Classifier
from sklearn.neighbors import KNeighborsClassifier
#Fitting KNN Classifier
knn= KNeighborsClassifier()
knn.fit(X_train, y_train)

#class prediction of y
y_pred_knn=knn.predict(X_test)
y_train_pred_knn=knn.predict(X_train)

#getting all scores for Support Vector Classifier
train_accuracy_knn = round(accuracy_score(y_train_pred_knn,y_train), 3)
accuracy_knn = round(accuracy_score(y_pred_knn,y_test), 3)
precision_score_knn = round(precision_score(y_pred_knn,y_test), 3)
recall_score_knn = round(recall_score(y_pred_knn,y_test), 3)
f1_score_knn = round(f1_score(y_pred_knn,y_test), 3)
roc_score_knn = round(roc_auc_score(y_pred_knn,y_test), 3)

print("The accuracy on train data is ", train_accuracy_knn)
print("The accuracy on test data is ", accuracy_knn)
print("The precision on test data is ", precision_score_knn)
print("The recall on test data is ", recall_score_knn)
print("The f1 on test data is ", f1_score_knn)
print("The roc_score on test data is ", roc_score_knn)

"""It is likely over-fitting, we need to perform Hyper-Parameter tuning and Cross-Validation.

# Hyper-Parameter Tuning and Cross Validation
"""

from sklearn.model_selection import GridSearchCV
k_range = list(range(1,10)) #creating a list of k values
param_grid = dict(n_neighbors=k_range)
grid = GridSearchCV(knn, param_grid, cv=3, scoring='roc_auc', return_train_score=False,verbose=1)
grid_search=grid.fit(X_train, y_train)

grid.best_estimator_

grid.best_params_

knn_optimal_model = grid.best_estimator_

#class prediction of y on train and test
y_pred_knn_grid=knn_optimal_model.predict(X_test)
y_train_pred_knn_grid=knn_optimal_model.predict(X_train)

#getting all scores for Support Vector Classifier after CV and Hyperparameter Tunning
train_accuracy_knn_grid = round(accuracy_score(y_train_pred_knn_grid,y_train), 3)
accuracy_knn_grid = round(accuracy_score(y_pred_knn_grid,y_test), 3)
precision_score_knn_grid = round(precision_score(y_pred_knn_grid,y_test), 3)
recall_score_knn_grid = round(recall_score(y_pred_knn_grid,y_test), 3)
f1_score_knn_grid = round(f1_score(y_pred_knn_grid,y_test), 3)
auc_knn_grid = round(roc_auc_score(y_pred_knn_grid,y_test), 3)

print("The accuracy on train data is ", train_accuracy_knn_grid)
print("The accuracy on test data is ", accuracy_knn_grid)
print("The precision on test data is ", precision_score_knn_grid)
print("The recall on test data is ", recall_score_knn_grid)
print("The f1 on test data is ", f1_score_knn_grid)
print("The auc on test data is ", auc_knn_grid)

"""# Gradient Boosting"""

#importing Gradient Boosting Classifier
from sklearn.ensemble import GradientBoostingClassifier
#fitting data into Gradient Boosting Classifier
gbc = GradientBoostingClassifier(random_state=42)
gbc.fit(X_train, y_train)

#class prediction of y
y_pred_gbc=gbc.predict(X_test)
y_train_pred_gbc=gbc.predict(X_train)

#getting all scores for Gradient Boosting Classifier
train_accuracy_gbc = round(accuracy_score(y_train_pred_gbc,y_train), 3)
accuracy_gbc = round(accuracy_score(y_pred_gbc,y_test), 3)
precision_score_gbc = round(precision_score(y_pred_gbc,y_test), 3)
recall_score_gbc = round(recall_score(y_pred_gbc,y_test), 3)
f1_score_gbc = round(f1_score(y_pred_gbc,y_test), 3)
roc_score_gbc = round(roc_auc_score(y_pred_gbc,y_test), 3)

print("The accuracy on train data is ", train_accuracy_gbc)
print("The accuracy on test data is ", accuracy_gbc)
print("The precision on test data is ", precision_score_gbc)
print("The recall on test data is ", recall_score_gbc)
print("The f1 on test data is ", f1_score_gbc)
print("The roc_score on test data is ", roc_score_gbc)

# Get the confusion matrix for Gradient Boosting Classifier
labels = ['Not Defaulter', 'Defaulter']
cm_gbc = confusion_matrix(y_test, y_pred_gbc )
print(cm_gbc)

#plot confusion matrix
ax= plt.subplot()
sns.heatmap(cm_gbc, annot=True, ax = ax)

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - Gradient Boosting Classifier')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""# XG Boosting"""

#importing XG Boosting Classifier
from xgboost import XGBClassifier
#fitting data into XG Boosting Classifier
xgb = XGBClassifier()
xgb.fit(X_train,y_train)

#class prediction of y
y_pred_xgb=xgb.predict(X_test)
y_train_pred_xgb=xgb.predict(X_train)

#getting all scores for XG Boosting Classifier
train_accuracy_xgb = round(accuracy_score(y_train_pred_xgb,y_train), 3)
accuracy_xgb = round(accuracy_score(y_pred_xgb,y_test), 3)
precision_score_xgb = round(precision_score(y_pred_xgb,y_test), 3)
recall_score_xgb = round(recall_score(y_pred_xgb,y_test), 3)
f1_score_xgb = round(f1_score(y_pred_xgb,y_test), 3)
roc_score_xgb = round(roc_auc_score(y_pred_xgb,y_test), 3)

print("The accuracy on train data is ", train_accuracy_xgb)
print("The accuracy on test data is ", accuracy_xgb)
print("The precision on test data is ", precision_score_xgb)
print("The recall on test data is ", recall_score_xgb)
print("The f1 on test data is ", f1_score_xgb)
print("The roc_score on test data is ", roc_score_xgb)

"""It is over-fitting, Hyper-Parameter tuning and Cross-Validation is to be done."""

# Get the confusion matrix for XG Boosting Classifier
labels = ['Not Defaulter', 'Defaulter']
cm_xgb = confusion_matrix(y_test, y_pred_xgb )
print(cm_xgb)

#plot confusion matrix
ax= plt.subplot()
sns.heatmap(cm_xgb, annot=True, ax = ax)

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix - XG Boosting Classifier')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

"""# Hyper-Parameter Tuning and Cross-Validation"""

from sklearn.model_selection import RandomizedSearchCV
# Hyperparameter Grid
param_dict = {'learning_rate': [0.15, 0.1, 0.05],'n_estimators' : [200, 250],'max_depth' : [15,20,25],'min_child_weight' : [1,3],'gamma': [0.3, 0.2, 0.1],'min_samples_leaf' : [40, 50]}

# Create an instance of the RandomForestClassifier
xgb = XGBClassifier()

# Grid search
xgb_grid = RandomizedSearchCV(estimator=xgb,
                       param_distributions = param_dict,n_jobs=-1, n_iter=5, cv =3, verbose=2, scoring='roc_auc')
# fitting model
xgb_grid.fit(X_train,y_train)

xgb_grid.best_estimator_

xgb_grid.best_params_

xgb_optimal_model = xgb_grid.best_estimator_

#class prediction of y on train and test
y_pred_xgb_grid=xgb_optimal_model.predict(X_test)
y_train_pred_xgb_grid=xgb_optimal_model.predict(X_train)

#getting all scores for XG Boosting after CV and Hyperparameter Tunning
train_accuracy_xgb_grid = round(accuracy_score(y_train_pred_xgb_grid,y_train), 3)
accuracy_xgb_grid = round(accuracy_score(y_pred_xgb_grid,y_test), 3)
precision_score_xgb_grid = round(precision_score(y_pred_xgb_grid,y_test), 3)
recall_score_xgb_grid = round(recall_score(y_pred_xgb_grid,y_test), 3)
f1_score_xgb_grid = round(f1_score(y_pred_xgb_grid,y_test), 3)
auc_xgb_grid = round(roc_auc_score(y_pred_xgb_grid,y_test), 3)

print("The accuracy on train data is ", train_accuracy_xgb_grid)
print("The accuracy on test data is ", accuracy_xgb_grid)
print("The precision on test data is ", precision_score_xgb_grid)
print("The recall on test data is ", recall_score_xgb_grid)
print("The f1 on test data is ", f1_score_xgb_grid)
print("The auc on test data is ", auc_xgb_grid)

"""It is still over-fitting

# **Conclusion**

1.   Decision Tree, Random Forest and XG Boost algorithm had overfitted and had to be Hyper-Parameter Tuned but XG Boost continued to over-fit even after hyper-parameter tuning.
2.   Gradient Boosting has given the best result with a accuracy score of 84% and ROC score of 0.855


3.   Cross validation and hyperparameter tunning certainly reduces chances of overfitting and also increases performance of model.
"""